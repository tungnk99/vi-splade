# Dataset Manager - Enhanced Version

Dataset Manager Ä‘Ã£ Ä‘Æ°á»£c nÃ¢ng cáº¥p vá»›i cÃ¡c tÃ­nh nÄƒng má»›i máº¡nh máº½:

## ğŸš€ TÃ­nh nÄƒng má»›i

### 1. Auto-Download tá»« Hugging Face Hub
- Tá»± Ä‘á»™ng download dataset tá»« HF náº¿u chÆ°a cÃ³ á»Ÿ local
- Há»— trá»£ authentication vá»›i HF_TOKEN
- Download song song vá»›i progress bar

### 2. Auto-Convert sang Ä‘á»‹nh dáº¡ng chuáº©n
- Tá»± Ä‘á»™ng convert dataset Ä‘Ã£ download sang Ä‘á»‹nh dáº¡ng VNLegalDataset
- Mapping thÃ´ng minh cÃ¡c format khÃ¡c nhau
- Táº¡o corpus vÃ  split files chuáº©n

### 3. Build Dataset theo format
- **Query-Document pairs**: Cho training retrieval models
- **Triplets**: Query + Positive + Negative docs
- **Reranking**: Query + Document + Relevance score

### 4. Quáº£n lÃ½ thÃ´ng qua configs
- Load dataset configurations tá»« JSON
- Mapping automatic giá»¯a use cases vÃ  converters
- Há»— trá»£ nhiá»u dataset sources

## ğŸ“– CÃ¡ch sá»­ dá»¥ng

### Basic Usage

```python
from data_process.dataset_manager import DatasetManager

# Khá»Ÿi táº¡o vá»›i auto features
manager = DatasetManager(
    auto_download=True,    # Tá»± Ä‘á»™ng download náº¿u thiáº¿u
    auto_convert=True      # Tá»± Ä‘á»™ng convert sang format chuáº©n
)

# Load dataset (sáº½ auto-download náº¿u cáº§n)
dataset = manager.load_dataset('vn_legal_retrieval', 'train')
print(f"Loaded {len(dataset)} samples")
```

### Building Different Formats

```python
# 1. Query-Document pairs
query_doc_datasets = manager.build_dataset(
    dataset_id='vn_legal_retrieval',
    format='query_doc',
    max_samples=1000
)

# 2. Triplets vá»›i negative sampling
triple_datasets = manager.build_dataset(
    dataset_id='vinli_triplet', 
    format='triple',
    negative_sampling=True,
    negative_ratio=2.0  # 2 negatives per positive
)

# 3. Reranking format
reranking_datasets = manager.build_dataset(
    dataset_id='vn_legal_retrieval',
    format='reranking'
)
```

### Advanced Usage

```python
# Kiá»ƒm tra dataset info
info = manager.get_dataset_info('vinli_triplet')
print(f"Available: {info['available']}")
print(f"Splits: {info.get('split_info', {}).keys()}")

# Get statistics
stats = manager.get_dataset_statistics('vn_legal_retrieval')
print(f"Total rows: {stats['total_rows']}")
print(f"Size: {stats['total_size_mb']} MB")

# List available datasets
available = manager.list_available_datasets()
print(f"Configured datasets: {available}")
```

## ğŸ”§ Configuration

Dataset configurations trong `dataset_configs.json`:

```json
{
  "datasets": {
    "vn_legal_retrieval": {
      "name": "Vietnamese Legal Document Retrieval Data",
      "description": "Vietnamese legal document retrieval dataset for question-answering",
      "hub_id": "YuITC/Vietnamese-Legal-Doc-Retrieval-Data",
      "splits": {
        "train": "train_data.parquet",
        "test": "test_data.parquet"
      },
      "local_path": "data/vn_legal_retrieval",
      "file_format": "parquet",
      "use_case": "legal_qa",
      "language": "vi"
    }
  },
  "download_settings": {
    "cache_dir": "data/.cache",
    "require_auth": true,
    "parallel_downloads": 4,
    "verify_checksums": true
  }
}
```

## ğŸ—ï¸ Dataset Formats

### Query-Document Pairs
```python
{
    'query': 'CÃ¢u há»i vá» luáº­t...',
    'document': 'Ná»™i dung tÃ i liá»‡u phÃ¡p lÃ½...',
    'label': 1  # Relevance score
}
```

### Triplets
```python
{
    'query': 'CÃ¢u há»i vá» luáº­t...',
    'positive': 'TÃ i liá»‡u liÃªn quan...',
    'negative': 'TÃ i liá»‡u khÃ´ng liÃªn quan...'
}
```

### Reranking
```python
{
    'query': 'CÃ¢u há»i vá» luáº­t...',
    'document': 'TÃ i liá»‡u Ä‘á»ƒ rank...',
    'score': 0.85  # Relevance score
}
```

## ğŸ¯ Use Cases

### 1. Training Retrieval Models
```python
# Láº¥y query-doc pairs cho training
datasets = manager.build_dataset('vn_legal_retrieval', format='query_doc')
train_data = datasets['train']

# Sá»­ dá»¥ng vá»›i PyTorch DataLoader
from torch.utils.data import DataLoader
loader = DataLoader(train_data, batch_size=32)
```

### 2. Training vá»›i Triplet Loss
```python
# Láº¥y triplets cho contrastive learning
triplets = manager.build_dataset('vinli_triplet', format='triple')
train_triplets = triplets['train']

# CÃ³ sáºµn query, positive, negative
for batch in train_triplets:
    query = batch['query']
    positive = batch['positive'] 
    negative = batch['negative']
    # Train model vá»›i triplet loss
```

### 3. Evaluation vá»›i Reranking
```python
# Láº¥y data vá»›i scores Ä‘á»ƒ evaluate
rerank_data = manager.build_dataset('vn_legal_retrieval', format='reranking')
test_data = rerank_data['test']

# Evaluate ranking performance
for item in test_data:
    predicted_score = model.predict(item['query'], item['document'])
    true_score = item['score']
    # Compute ranking metrics
```

## ğŸ”„ Auto-Download Flow

1. **Check Local**: Kiá»ƒm tra dataset cÃ³ sáºµn local khÃ´ng
2. **Download**: Náº¿u khÃ´ng cÃ³ vÃ  `auto_download=True`, download tá»« HF
3. **Convert**: Náº¿u cÃ³ raw data vÃ  `auto_convert=True`, convert sang format chuáº©n
4. **Load**: Load dataset Ä‘Ã£ convert

## ğŸŒŸ Environment Setup

```bash
# Set HF token (optional, for private datasets)
export HF_TOKEN="your_huggingface_token"

# Or create .env file
echo "HF_TOKEN=your_huggingface_token" > .env
```

## ğŸ“ Examples

Xem file `examples/example_dataset_manager.py` Ä‘á»ƒ demo Ä‘áº§y Ä‘á»§ cÃ¡c tÃ­nh nÄƒng:

```bash
python examples/example_dataset_manager.py
```

## ğŸš€ Quick Start

```python
# All-in-one example
from data_process.dataset_manager import DatasetManager

manager = DatasetManager(auto_download=True, auto_convert=True)

# Build training data
train_data = manager.build_dataset(
    'vn_legal_retrieval', 
    format='query_doc',
    splits=['train']
)['train']

# Ready to use!
print(f"Training data: {len(train_data)} samples")
print(f"Sample: {train_data[0]}")
```

Vá»›i Dataset Manager nÃ¢ng cáº¥p, viá»‡c quáº£n lÃ½ vÃ  sá»­ dá»¥ng datasets trá»Ÿ nÃªn Ä‘Æ¡n giáº£n vÃ  linh hoáº¡t hÆ¡n ráº¥t nhiá»u! ğŸ‰